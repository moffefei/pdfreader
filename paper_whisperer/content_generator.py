"""
å†…å®¹ç”Ÿæˆæ¨¡å—
ç”Ÿæˆå…¬ä¼—å·æ–‡ç« ï¼ˆMarkdownï¼‰å’Œå°çº¢ä¹¦ç¬”è®°å†…å®¹
"""
from typing import Dict
from paper_whisperer.llm_client import LLMClientFactory


class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_provider: str = "openai"):
        """
        åˆå§‹åŒ–å†…å®¹ç”Ÿæˆå™¨
        
        Args:
            llm_provider: LLM æä¾›å•†
        """
        self.llm_client = LLMClientFactory.create_client(provider=llm_provider)
    
    def generate_wechat_article(self, analysis_result: Dict) -> str:
        """
        ç”Ÿæˆå…¬ä¼—å·æ–‡ç« ï¼ˆMarkdown æ ¼å¼ï¼‰
        
        Args:
            analysis_result: è®ºæ–‡åˆ†æç»“æœ
            
        Returns:
            Markdown æ ¼å¼çš„æ–‡ç« å†…å®¹
        """
        key_info = analysis_result.get("key_info", {})
        summary = analysis_result.get("summary", "")
        metadata = analysis_result.get("metadata", {})
        
        prompt = f"""åŸºäºä»¥ä¸‹è®ºæ–‡åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ç¯‡é€‚åˆå…¬ä¼—å·å‘å¸ƒçš„ç§‘æ™®æ–‡ç« ï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚

è¦æ±‚ï¼š
1. æ ‡é¢˜è¦å¸å¼•äººï¼Œèƒ½å¼•èµ·è¯»è€…å…´è¶£
2. å¼€å¤´è¦æœ‰å¼•äººå…¥èƒœçš„å¼•è¨€
3. å†…å®¹è¦é€šä¿—æ˜“æ‡‚ï¼Œé¿å…è¿‡äºä¸“ä¸šçš„æœ¯è¯­
4. é€‚å½“ä½¿ç”¨å°æ ‡é¢˜åˆ†æ®µ
5. çªå‡ºè®ºæ–‡çš„åˆ›æ–°ç‚¹å’Œåº”ç”¨ä»·å€¼
6. ç»“å°¾è¦æœ‰æ€»ç»“å’Œæ€è€ƒ

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜: {key_info.get('title', metadata.get('title', ''))}
ä½œè€…: {', '.join(key_info.get('authors', []))}
æ‘˜è¦: {key_info.get('abstract', '')}
ä¸»è¦è´¡çŒ®: {', '.join(key_info.get('main_contributions', []))}
ç ”ç©¶æ–¹æ³•: {key_info.get('methodology', '')}
ä¸»è¦ç»“æœ: {key_info.get('main_results', '')}

æ·±åº¦è§£è¯»æ‘˜è¦ï¼š
{summary}

è¯·ç”Ÿæˆå®Œæ•´çš„ Markdown æ–‡ç« ï¼ŒåŒ…å«æ ‡é¢˜ã€å¼•è¨€ã€æ­£æ–‡ã€æ€»ç»“ç­‰éƒ¨åˆ†ã€‚"""
        
        try:
            article = self.llm_client.chat_completion([
                {"role": "user", "content": prompt}
            ], temperature=0.8, max_tokens=3000)
        except Exception as e:
            article = f"ç”Ÿæˆæ–‡ç« æ—¶å‡ºé”™: {str(e)}"
        
        return article
    
    def generate_xiaohongshu_note(self, analysis_result: Dict) -> str:
        """
        ç”Ÿæˆå°çº¢ä¹¦ç¬”è®°å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰
        
        Args:
            analysis_result: è®ºæ–‡åˆ†æç»“æœ
            
        Returns:
            Markdown æ ¼å¼çš„å°çº¢ä¹¦ç¬”è®°å†…å®¹
        """
        key_info = analysis_result.get("key_info", {})
        summary = analysis_result.get("summary", "")
        
        prompt = f"""åŸºäºä»¥ä¸‹è®ºæ–‡åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ç¯‡é€‚åˆå°çº¢ä¹¦å‘å¸ƒçš„ç¬”è®°å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚

è¦æ±‚ï¼š
1. æ ‡é¢˜è¦å¸å¼•çœ¼çƒï¼Œå¯ä»¥ä½¿ç”¨ emoji è£…é¥°
2. å¼€å¤´è¦æœ‰å¸å¼•äººçš„ hookï¼ˆé’©å­ï¼‰
3. ä½¿ç”¨è¦ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªè¦ç‚¹å‰åŠ  emoji
4. è¯­è¨€è¦è½»æ¾æ´»æ³¼ï¼Œä½†ä¿æŒä¸“ä¸šæ€§
5. é€‚å½“ä½¿ç”¨ emoji å¢åŠ è¶£å‘³æ€§
6. å†…å®¹è¦ç®€æ´ï¼Œæ§åˆ¶åœ¨ 500-800 å­—
7. ç»“å°¾è¦æœ‰äº’åŠ¨å¼•å¯¼ï¼ˆå¦‚"ä½ è§‰å¾—å‘¢ï¼Ÿ"ï¼‰

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜: {key_info.get('title', '')}
ä¸»è¦è´¡çŒ®: {', '.join(key_info.get('main_contributions', []))}
ä¸»è¦ç»“æœ: {key_info.get('main_results', '')}

æ·±åº¦è§£è¯»æ‘˜è¦ï¼š
{summary}

è¯·ç”Ÿæˆå®Œæ•´çš„ Markdown ç¬”è®°ï¼Œæ ¼å¼è¦ç¬¦åˆå°çº¢ä¹¦é£æ ¼ï¼Œä½¿ç”¨é€‚å½“çš„ emojiã€‚"""
        
        try:
            note = self.llm_client.chat_completion([
                {"role": "user", "content": prompt}
            ], temperature=0.9, max_tokens=2000)
        except Exception as e:
            note = f"ç”Ÿæˆç¬”è®°æ—¶å‡ºé”™: {str(e)}"
        
        return note
    
    def generate_xiaohongshu_note_structured(self, analysis_result: Dict) -> Dict:
        """
        ç”Ÿæˆç»“æ„åŒ–çš„å°çº¢ä¹¦ç¬”è®°å†…å®¹
        
        Args:
            analysis_result: è®ºæ–‡åˆ†æç»“æœ
            
        Returns:
            åŒ…å«æ ‡é¢˜ã€è¦ç‚¹åˆ—è¡¨ç­‰ç»“æ„åŒ–å†…å®¹çš„å­—å…¸
        """
        key_info = analysis_result.get("key_info", {})
        summary = analysis_result.get("summary", "")
        
        prompt = f"""åŸºäºä»¥ä¸‹è®ºæ–‡åˆ†æç»“æœï¼Œç”Ÿæˆå°çº¢ä¹¦ç¬”è®°çš„ç»“æ„åŒ–å†…å®¹ï¼Œä»¥ JSON æ ¼å¼è¿”å›ï¼š

{{
    "title": "å¸å¼•äººçš„æ ‡é¢˜ï¼ˆå¯å«emojiï¼‰",
    "hook": "å¼€å¤´å¸å¼•äººçš„ä¸€å¥è¯",
    "key_points": [
        "è¦ç‚¹1ï¼ˆå¯å«emojiï¼‰",
        "è¦ç‚¹2ï¼ˆå¯å«emojiï¼‰",
        ...
    ],
    "highlight": "æ ¸å¿ƒäº®ç‚¹ï¼ˆ1-2å¥è¯ï¼‰",
    "conclusion": "æ€»ç»“å’Œäº’åŠ¨å¼•å¯¼"
}}

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜: {key_info.get('title', '')}
ä¸»è¦è´¡çŒ®: {', '.join(key_info.get('main_contributions', []))}
ä¸»è¦ç»“æœ: {key_info.get('main_results', '')}

æ·±åº¦è§£è¯»æ‘˜è¦ï¼š
{summary}

è¯·åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
        
        try:
            response = self.llm_client.chat_completion([
                {"role": "user", "content": prompt}
            ], temperature=0.9, max_tokens=1500)
            
            # è§£æ JSON
            response = response.strip()
            if response.startswith("```json"):
                response = response[7:]
            if response.startswith("```"):
                response = response[3:]
            if response.endswith("```"):
                response = response[:-3]
            response = response.strip()
            
            import json
            structured_note = json.loads(response)
        except Exception as e:
            print(f"ç”Ÿæˆç»“æ„åŒ–ç¬”è®°å¤±è´¥: {e}")
            structured_note = {
                "title": f"ğŸ“š {key_info.get('title', 'è®ºæ–‡è§£è¯»')}",
                "hook": "ä»Šå¤©æ¥èŠèŠè¿™ç¯‡æœ‰è¶£çš„è®ºæ–‡ï¼",
                "key_points": [
                    f"âœ¨ ä¸»è¦è´¡çŒ®: {', '.join(key_info.get('main_contributions', []))}",
                    f"ğŸ”¬ ç ”ç©¶æ–¹æ³•: {key_info.get('methodology', '')[:100]}",
                    f"ğŸ“Š ä¸»è¦ç»“æœ: {key_info.get('main_results', '')[:100]}"
                ],
                "highlight": summary[:200] if summary else "è¿™æ˜¯ä¸€ç¯‡å€¼å¾—å…³æ³¨çš„è®ºæ–‡",
                "conclusion": "ä½ è§‰å¾—è¿™ä¸ªç ”ç©¶æ€ä¹ˆæ ·ï¼Ÿæ¬¢è¿åœ¨è¯„è®ºåŒºè®¨è®ºï¼"
            }
        
        return structured_note

